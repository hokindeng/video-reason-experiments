==========================================
Job ID: 1802707
Job Name: hunyuan-G1
Node: gh014
Start Time: Sun 11 Jan 2026 06:53:02 PM CST
Working Directory: /u/yli8/hokin/video-reason-experiments/jobs
==========================================

Loaded modules:

GPU Information:
Sun Jan 11 18:53:03 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GH200 120GB             On  |   00000009:01:00.0 Off |                    0 |
| N/A   20C    P0             84W /  900W |      61MiB /  97871MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Starting inference...
Model: hunyuan-video-i2v
Task: G-1_object_trajectory_data-generator
Questions dir: ./data/questions/G-1_object_trajectory_data-generator

üéØ VMEvalKit Inference Runner
Model: hunyuan-video-i2v
GPU: 0
Questions dir: /u/yli8/hokin/video-reason-experiments/data/questions/G-1_object_trajectory_data-generator
Output dir: /u/yli8/hokin/video-reason-experiments/data/outputs

üöÄ Running: python /u/yli8/hokin/video-reason-experiments/VMEvalKit/examples/generate_videos.py --model hunyuan-video-i2v --questions-dir /u/yli8/hokin/video-reason-experiments/data/questions/G-1_object_trajectory_data-generator --output-dir /u/yli8/hokin/video-reason-experiments/data/outputs --gpu 0
Working directory: /u/yli8/hokin/video-reason-experiments/VMEvalKit
================================================================================
üêç Activating venv: /u/yli8/hokin/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v
üéÆ Using GPU 0 (CUDA_VISIBLE_DEVICES=0)
üîç Discovering human-approved tasks from folder structure...
üîç Discovering tasks from: /u/yli8/hokin/video-reason-experiments/data/questions/G-1_object_trajectory_data-generator
   üìÅ Scanning object_trajectory_task/
      ‚úÖ Found 50 tasks in object_trajectory

üìä Discovery Summary: 50 total tasks
   object_trajectory: 50 tasks
   üéØ Running all discovered tasks

üéØ Selected 1 model(s): hunyuan-video-i2v

üîç Verifying 1 model(s) for testing...
   ‚úÖ hunyuan-video-i2v: HunyuanVideo
üöÄ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

üìä Experiment Configuration:
   Models to run: 1 - hunyuan-video-i2v
   Domains: 1
   üîÑ Execution Mode: SEQUENTIAL

üìà Task Distribution:
   Object_Trajectory: 50 approved tasks
   Total approved tasks: 50
   Total generations: 50

üìÅ Output directory: /u/yli8/hokin/video-reason-experiments/data/outputs
   Skip existing: True

üìÅ Output directory structure ready at: /u/yli8/hokin/video-reason-experiments/data/outputs
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
üìÅ Created per-model folders under: /u/yli8/hokin/video-reason-experiments/data/outputs and mirrored question tasks
üìã Total inference jobs to run: 50
üöÄ Starting sequential execution...

   Processing order: Model by model, task by task

ü§ñ Processing Model: HunyuanVideo (hunyuan-video-i2v)

  üìö Domain: Object_Trajectory
    [1/50] Processing: object_trajectory_0000

  üé¨ Generating: object_trajectory_0000 with hunyuan-video-i2v
     Image: /u/yli8/hokin/video-reason-experiments/data/questions/G-1_object_trajectory_data-generator/object_trajectory_task/object_trajectory_0000/first_frame.png
     Prompt: The scene contains a circle object and a dashed target position (indicated by a ...
Using model-specific Python: /u/yli8/hokin/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/bin/python
üì¶ Loaded model: hunyuan-video-i2v (will be reused for all tasks)
Warning: HunyuanVideo requires 60-80GB GPU memory for 720p generation

‚úÖ Inference complete! Output saved to: /u/yli8/hokin/video-reason-experiments/data/outputs/hunyuan-video-i2v/object_trajectory_task/object_trajectory_0000
   - Generated: /u/yli8/hokin/video-reason-experiments/data/outputs/hunyuan-video-i2v/object_trajectory_task/object_trajectory_0000/video/video.mp4
   - Question: /u/yli8/hokin/video-reason-experiments/data/outputs/hunyuan-video-i2v/object_trajectory_task/object_trajectory_0000/question/
     ‚ùå Failed: 2026-01-11 18:53:18.584 | INFO     | hyvideo.inference:from_pretrained:220 - Got text-to-video model root path: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/hunyuan-video-i2v-720p
2026-01-11 18:53:19.647 | INFO     | hyvideo.inference:from_pretrained:258 - Building model...
2026-01-11 18:53:20.522 | INFO     | hyvideo.inference:load_state_dict:494 - Loading torch model ckpts/hunyuan-video-i2v-720p/transformers/mp_rank_00_model_states.pt...
2026-01-11 18:53:44.660 | INFO     | hyvideo.vae:load_vae:29 - Loading 3D VAE model (884-16c-hy) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/hunyuan-video-i2v-720p/vae
2026-01-11 18:53:46.540 | INFO     | hyvideo.vae:load_vae:55 - VAE to dtype: torch.float16
2026-01-11 18:53:46.548 | INFO     | hyvideo.text_encoder:load_text_encoder:35 - Loading text encoder model (llm-i2v) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/text_encoder_i2v

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:11<00:34, 11.34s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:13<00:12,  6.21s/it]
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:17<00:05,  5.11s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:48<00:00, 15.25s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:48<00:00, 12.14s/it]
2026-01-11 18:54:37.136 | INFO     | hyvideo.text_encoder:load_text_encoder:61 - Text encoder to dtype: torch.float16
2026-01-11 18:54:37.140 | INFO     | hyvideo.text_encoder:load_tokenizer:75 - Loading tokenizer (llm-i2v) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/text_encoder_i2v
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2026-01-11 18:54:37.373 | INFO     | hyvideo.text_encoder:load_text_encoder:35 - Loading text encoder model (clipL) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/text_encoder_2
2026-01-11 18:54:38.817 | INFO     | hyvideo.text_encoder:load_text_encoder:61 - Text encoder to dtype: torch.float16
2026-01-11 18:54:38.818 | INFO     | hyvideo.text_encoder:load_tokenizer:75 - Loading tokenizer (clipL) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/text_encoder_2
2026-01-11 18:54:39.053 | INFO     | hyvideo.inference:predict:771 - Input (height, width, video_length) = (1280, 720, 129)
2026-01-11 18:54:43.271 | DEBUG    | hyvideo.inference:get_rotary_pos_embed:690 - actual_num_frames = 129 <= 192, using original RoPE
2026-01-11 18:54:43.317 | DEBUG    | hyvideo.inference:get_rotary_pos_embed:698 - freqs_cos shape: torch.Size([118800, 128]), device: cpu
2026-01-11 18:54:43.317 | DEBUG    | hyvideo.inference:predict:886 - 
                        height: 960
                         width: 960
                  video_length: 129
                        prompt: ['The scene contains a circle object and a dashed target position (indicated by a dashed outline of the same shape). Keep the dashed target position unchanged. Move the circle object to the dashed target position along the shortest path, ensuring it completely overlaps with the target.']
                    neg_prompt: ['']
                          seed: 0
                   infer_steps: 50
         num_videos_per_prompt: 1
                guidance_scale: 1.0
                      n_tokens: 118800
                    flow_shift: 7.0
       embedded_guidance_scale: 6.0
                 i2v_stability: True

  0%|          | 0/50 [00:00<?, ?it/s]
  0%|          | 0/50 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 65, in <module>
    main()
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 32, in main
    outputs = hunyuan_video_sampler.predict(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/inference.py", line 889, in predict
    samples = self.pipeline(
              ^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/pipeline_hunyuan_video.py", line 1048, in __call__
    noise_pred = self.transformer(  # For an input image (129, 192, 336) (1, 256, 256)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/models.py", line 763, in forward
    img, txt = block(*double_block_args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/models.py", line 233, in forward
    attn = attention(
           ^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/attenion.py", line 108, in attention
    x = flash_attn_varlen_func(
        ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not callable

      ‚ùå Failed: 2026-01-11 18:53:18.584 | INFO     | hyvideo.inference:from_pretrained:220 - Got text-to-video model root path: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/hunyuan-video-i2v-720p
2026-01-11 18:53:19.647 | INFO     | hyvideo.inference:from_pretrained:258 - Building model...
2026-01-11 18:53:20.522 | INFO     | hyvideo.inference:load_state_dict:494 - Loading torch model ckpts/hunyuan-video-i2v-720p/transformers/mp_rank_00_model_states.pt...
2026-01-11 18:53:44.660 | INFO     | hyvideo.vae:load_vae:29 - Loading 3D VAE model (884-16c-hy) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/hunyuan-video-i2v-720p/vae
2026-01-11 18:53:46.540 | INFO     | hyvideo.vae:load_vae:55 - VAE to dtype: torch.float16
2026-01-11 18:53:46.548 | INFO     | hyvideo.text_encoder:load_text_encoder:35 - Loading text encoder model (llm-i2v) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/text_encoder_i2v

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:11<00:34, 11.34s/it]
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:13<00:12,  6.21s/it]
Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:17<00:05,  5.11s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:48<00:00, 15.25s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:48<00:00, 12.14s/it]
2026-01-11 18:54:37.136 | INFO     | hyvideo.text_encoder:load_text_encoder:61 - Text encoder to dtype: torch.float16
2026-01-11 18:54:37.140 | INFO     | hyvideo.text_encoder:load_tokenizer:75 - Loading tokenizer (llm-i2v) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/text_encoder_i2v
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2026-01-11 18:54:37.373 | INFO     | hyvideo.text_encoder:load_text_encoder:35 - Loading text encoder model (clipL) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/text_encoder_2
2026-01-11 18:54:38.817 | INFO     | hyvideo.text_encoder:load_text_encoder:61 - Text encoder to dtype: torch.float16
2026-01-11 18:54:38.818 | INFO     | hyvideo.text_encoder:load_tokenizer:75 - Loading tokenizer (clipL) from: /u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/text_encoder_2
2026-01-11 18:54:39.053 | INFO     | hyvideo.inference:predict:771 - Input (height, width, video_length) = (1280, 720, 129)
2026-01-11 18:54:43.271 | DEBUG    | hyvideo.inference:get_rotary_pos_embed:690 - actual_num_frames = 129 <= 192, using original RoPE
2026-01-11 18:54:43.317 | DEBUG    | hyvideo.inference:get_rotary_pos_embed:698 - freqs_cos shape: torch.Size([118800, 128]), device: cpu
2026-01-11 18:54:43.317 | DEBUG    | hyvideo.inference:predict:886 - 
                        height: 960
                         width: 960
                  video_length: 129
                        prompt: ['The scene contains a circle object and a dashed target position (indicated by a dashed outline of the same shape). Keep the dashed target position unchanged. Move the circle object to the dashed target position along the shortest path, ensuring it completely overlaps with the target.']
                    neg_prompt: ['']
                          seed: 0
                   infer_steps: 50
         num_videos_per_prompt: 1
                guidance_scale: 1.0
                      n_tokens: 118800
                    flow_shift: 7.0
       embedded_guidance_scale: 6.0
                 i2v_stability: True

  0%|          | 0/50 [00:00<?, ?it/s]
  0%|          | 0/50 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 65, in <module>
    main()
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 32, in main
    outputs = hunyuan_video_sampler.predict(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/inference.py", line 889, in predict
    samples = self.pipeline(
              ^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/pipeline_hunyuan_video.py", line 1048, in __call__
    noise_pred = self.transformer(  # For an input image (129, 192, 336) (1, 256, 256)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/models.py", line 763, in forward
    img, txt = block(*double_block_args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/models.py", line 233, in forward
    attn = attention(
           ^^^^^^^^^^
  File "/u/yli8/hokin/video-reason-experiments/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/modules/attenion.py", line 108, in attention
    x = flash_attn_varlen_func(
        ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not callable

    [2/50] Processing: object_trajectory_0001

  üé¨ Generating: object_trajectory_0001 with hunyuan-video-i2v
     Image: /u/yli8/hokin/video-reason-experiments/data/questions/G-1_object_trajectory_data-generator/object_trajectory_task/object_trajectory_0001/first_frame.png
     Prompt: The scene contains a square object and a dashed target position (indicated by a ...
Warning: HunyuanVideo requires 60-80GB GPU memory for 720p generation
