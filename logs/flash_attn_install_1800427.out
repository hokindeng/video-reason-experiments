════════════════════════════════════════════════════════════════
Flash-Attention Installation on Compute Node
════════════════════════════════════════════════════════════════
Job ID: 1800427
Node: gh141.hsn.cm.delta.internal.ncsa.edu
Started: Sun 11 Jan 2026 03:09:14 AM CST

Loading compatible compiler...
Current GCC: gcc (SUSE Linux) 7.5.0
Finding GCC 13.2 path...
✓ Found GCC 13.x at: /usr/bin/gcc-13
After setup:
  CC: /usr/bin/gcc-13
  GCC: gcc-13 (SUSE Linux) 13.2.1 20240206 [revision 67ac78caf31f7cb3202177e6428a46d829b70f23]
✓ GCC version 13 is compatible
  NVCC: Cuda compilation tools, release 12.6, V12.6.68
  CUDA_HOME: /sw/user/cudatoolkits/installs/cuda-12.6.1

GPU on this node:
GPU 0: NVIDIA GH200 120GB (UUID: GPU-c67a1128-dded-c31e-e82e-53b1ea3c54bb)

✓ Activated: /u/yli8/hokin/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v
  Python: Python 3.12.8

PyTorch: 2.7.0.dev20250224+cu126
CUDA: 12.6
GPU available: True

Installing build dependencies...
✓ Build dependencies installed

Build configuration:
  MAX_JOBS: 4
  TORCH_CUDA_ARCH_LIST: 8.0;9.0
  CUDA_HOME: /sw/user/cudatoolkits/installs/cuda-12.6.1

════════════════════════════════════════════════════════════════
Compiling flash-attention from source...
════════════════════════════════════════════════════════════════
This typically takes 15-30 minutes.
Compilation started: Sun 11 Jan 2026 03:09:37 AM CST

Using pip 25.3 from /u/yli8/hokin/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages/pip (python 3.12)
Collecting flash-attn==2.7.4.post1
  Using cached flash_attn-2.7.4.post1-cp312-cp312-linux_aarch64.whl
Requirement already satisfied: torch in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from flash-attn==2.7.4.post1) (2.7.0.dev20250224+cu126)
Requirement already satisfied: einops in ./VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from flash-attn==2.7.4.post1) (0.7.0)
Requirement already satisfied: filelock in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.16.1)
Requirement already satisfied: typing-extensions>=4.10.0 in ./VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (4.15.0)
Requirement already satisfied: setuptools in ./VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (80.9.0)
Requirement already satisfied: sympy==1.13.3 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (1.13.3)
Requirement already satisfied: networkx in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.4.2)
Requirement already satisfied: jinja2 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.1.5)
Requirement already satisfied: fsspec in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (2024.10.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from sympy==1.13.3->torch->flash-attn==2.7.4.post1) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from jinja2->torch->flash-attn==2.7.4.post1) (3.0.2)
Installing collected packages: flash-attn
Successfully installed flash-attn-2.7.4.post1

Compilation finished: Sun 11 Jan 2026 03:09:41 AM CST

════════════════════════════════════════════════════════════════
Verifying installation...
════════════════════════════════════════════════════════════════
✓ flash_attn version: 2.7.4.post1
✓ Location: /u/yli8/hokin/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages/flash_attn/__init__.py
✓ flash_attn_func imported successfully
✓ GPU: NVIDIA GH200 120GB
✓ Flash-attention is ready for GPU operations

============================================================
Flash-Attention installation SUCCESSFUL!
============================================================

════════════════════════════════════════════════════════════════
✅ Installation Complete!
════════════════════════════════════════════════════════════════
Job ID: 1800427
Finished: Sun 11 Jan 2026 03:09:44 AM CST

Flash-attention is now available in:
  /u/yli8/hokin/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v

Your HunyuanVideo jobs will automatically use flash-attention
for faster video generation.

