════════════════════════════════════════════════════════════════
Flash-Attention Installation on Compute Node
════════════════════════════════════════════════════════════════
Job ID: 1799390
Node: gh008.hsn.cm.delta.internal.ncsa.edu
Started: Sat 10 Jan 2026 07:31:12 PM CST

Loading compatible compiler...
Current GCC: gcc (Spack GCC) 14.2.0
Finding GCC 13.2 path...
✓ Found GCC 13.x at: /usr/bin/gcc-13
After setup:
  CC: /usr/bin/gcc-13
  GCC: gcc-13 (SUSE Linux) 13.2.1 20240206 [revision 67ac78caf31f7cb3202177e6428a46d829b70f23]
✓ GCC version 13 is compatible
  NVCC: Cuda compilation tools, release 12.6, V12.6.68
  CUDA_HOME: /sw/user/cudatoolkits/installs/cuda-12.6.1

GPU on this node:
GPU 0: NVIDIA GH200 120GB (UUID: GPU-6748a0c6-6ba9-a278-30fe-86164e5f9511)

✓ Activated: /u/yli8/hokin/video-reason-experiments/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v
  Python: Python 3.12.8

PyTorch: 2.7.0.dev20250224+cu126
CUDA: 12.6
GPU available: True

Installing build dependencies...
✓ Build dependencies installed

Build configuration:
  MAX_JOBS: 4
  TORCH_CUDA_ARCH_LIST: 8.0;9.0
  CUDA_HOME: /sw/user/cudatoolkits/installs/cuda-12.6.1

════════════════════════════════════════════════════════════════
Compiling flash-attention from source...
════════════════════════════════════════════════════════════════
This typically takes 15-30 minutes.
Compilation started: Sat 10 Jan 2026 07:31:27 PM CST

Using pip 25.3 from /u/yli8/hokin/video-reason-experiments/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages/pip (python 3.12)
Collecting flash-attn==2.7.4.post1
  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: torch in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from flash-attn==2.7.4.post1) (2.7.0.dev20250224+cu126)
Requirement already satisfied: einops in ./video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from flash-attn==2.7.4.post1) (0.7.0)
Requirement already satisfied: filelock in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.16.1)
Requirement already satisfied: typing-extensions>=4.10.0 in ./video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (4.15.0)
Requirement already satisfied: setuptools in ./video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (80.9.0)
Requirement already satisfied: sympy==1.13.3 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (1.13.3)
Requirement already satisfied: networkx in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.4.2)
Requirement already satisfied: jinja2 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.1.5)
Requirement already satisfied: fsspec in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (2024.10.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from sympy==1.13.3->torch->flash-attn==2.7.4.post1) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from jinja2->torch->flash-attn==2.7.4.post1) (3.0.2)
Building wheels for collected packages: flash-attn
  Building wheel for flash-attn (pyproject.toml): started
  Building wheel for flash-attn (pyproject.toml): finished with status 'done'
  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp312-cp312-linux_aarch64.whl size=187468090 sha256=1437d3e37b4c08862b7c9ae09c84d5b1814736024e3979203e4aa6abd2643f07
  Stored in directory: /work/nvme/bdqf/william/.cache/pip/wheels/48/0f/98/166bf6b39dd0592924b22ce21b799fc9fd76c3a302c12ee359
Successfully built flash-attn
Installing collected packages: flash-attn
Successfully installed flash-attn-2.7.4.post1

Compilation finished: Sat 10 Jan 2026 08:28:45 PM CST

════════════════════════════════════════════════════════════════
Verifying installation...
════════════════════════════════════════════════════════════════
✓ flash_attn version: 2.7.4.post1
✓ Location: /u/yli8/hokin/video-reason-experiments/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages/flash_attn/__init__.py
✓ flash_attn_func imported successfully
✓ GPU: NVIDIA GH200 120GB
✓ Flash-attention is ready for GPU operations

============================================================
Flash-Attention installation SUCCESSFUL!
============================================================

════════════════════════════════════════════════════════════════
✅ Installation Complete!
════════════════════════════════════════════════════════════════
Job ID: 1799390
Finished: Sat 10 Jan 2026 08:28:50 PM CST

Flash-attention is now available in:
  /u/yli8/hokin/video-reason-experiments/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v

Your HunyuanVideo jobs will automatically use flash-attention
for faster video generation.

