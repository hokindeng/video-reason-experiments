════════════════════════════════════════════════════════════════
Flash-Attention Installation on Compute Node
════════════════════════════════════════════════════════════════
Job ID: 1799302
Node: gh151.hsn.cm.delta.internal.ncsa.edu
Started: Sat 10 Jan 2026 06:07:35 PM CST

Loading compatible compiler...

Activating Modules:
  1) cray-libsci/24.07.0     2) cray-mpich/8.1.30


Environment:
  GCC: gcc (SUSE Linux) 7.5.0
  NVCC: Cuda compilation tools, release 12.6, V12.6.68
  CUDA_HOME: /sw/user/cudatoolkits/installs/cuda-12.6.1

GPU on this node:
GPU 0: NVIDIA GH200 120GB (UUID: GPU-e3a1e198-8147-8520-c91c-c9f80e59fb48)

✓ Activated: /u/yli8/hokin/video-reason-experiments/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v
  Python: Python 3.12.8

PyTorch: 2.7.0.dev20250224+cu126
CUDA: 12.6
GPU available: True

Installing build dependencies...
✓ Build dependencies installed

Build configuration:
  MAX_JOBS: 8
  TORCH_CUDA_ARCH_LIST: 8.0;9.0
  CUDA_HOME: /sw/user/cudatoolkits/installs/cuda-12.6.1

════════════════════════════════════════════════════════════════
Compiling flash-attention from source...
════════════════════════════════════════════════════════════════
This typically takes 15-30 minutes.
Compilation started: Sat 10 Jan 2026 06:07:43 PM CST

Using pip 25.3 from /u/yli8/hokin/video-reason-experiments/video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages/pip (python 3.12)
Collecting flash-attn==2.7.4.post1
  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: torch in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from flash-attn==2.7.4.post1) (2.7.0.dev20250224+cu126)
Requirement already satisfied: einops in ./video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from flash-attn==2.7.4.post1) (0.7.0)
Requirement already satisfied: filelock in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.16.1)
Requirement already satisfied: typing-extensions>=4.10.0 in ./video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (4.15.0)
Requirement already satisfied: setuptools in ./video-reason-experiments/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (80.9.0)
Requirement already satisfied: sympy==1.13.3 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (1.13.3)
Requirement already satisfied: networkx in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.4.2)
Requirement already satisfied: jinja2 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (3.1.5)
Requirement already satisfied: fsspec in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from torch->flash-attn==2.7.4.post1) (2024.10.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from sympy==1.13.3->torch->flash-attn==2.7.4.post1) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/user/python/miniforge3-pytorch-24.11.3/lib/python3.12/site-packages (from jinja2->torch->flash-attn==2.7.4.post1) (3.0.2)
Building wheels for collected packages: flash-attn
  Building wheel for flash-attn (pyproject.toml): started
