# Video Reasoning Experiments on Grace-Hopper HPC

Production-ready deployment of HunyuanVideo-I2V on ARM64 Grace-Hopper (GH200) HPC cluster with flash-attention acceleration.

## ğŸš€ Quick Start

```bash
# 1. Clone and initialize
git clone https://github.com/hokindeng/video-reason-experiments.git
cd video-reason-experiments
git submodule update --init --recursive

# 2. Setup environment + download models (~40 min)
bash jobs/setup_hunyuan_only.sh
sbatch jobs/install_flash_attention.slurm

# 3. Launch all 50 inference jobs
./jobs/submit_all_hunyuan.sh
```

**See [QUICKSTART.md](QUICKSTART.md) for detailed instructions.**

---

## âœ¨ Features

- âœ… **Flash-Attention 2** - 40-50% faster video generation on ARM64
- âœ… **Grace-Hopper Optimized** - Native ARM64 + Hopper GPU (sm_90)
- âœ… **SLURM Integration** - Batch job system for 50 parallel tasks
- âœ… **S3 Sync** - Automatic dataset download and result upload
- âœ… **Reproducible** - All dependencies pinned with exact versions

---

## ğŸ“Š System Requirements

### Hardware
- **Architecture**: ARM64 (aarch64) - Grace-Hopper GH200
- **GPU**: NVIDIA GH200 120GB (Hopper, sm_90)
- **Memory**: 64GB+ RAM per job
- **Storage**: ~50GB for environment + checkpoints

### Software  
- **OS**: Linux (tested on SLES 15)
- **CUDA**: 12.6+
- **Python**: 3.12+
- **SLURM**: Any recent version

---

## ğŸ“ Repository Structure

```
video-reason-experiments/
â”œâ”€â”€ QUICKSTART.md              # Start here!
â”œâ”€â”€ AGENT_HISTORY.md           # Complete deployment history
â”œâ”€â”€ README.md                  # This file
â”‚
â”œâ”€â”€ jobs/                      # SLURM jobs and setup scripts
â”‚   â”œâ”€â”€ README.md              # Jobs directory guide
â”‚   â”œâ”€â”€ FLASH_ATTENTION_SUCCESS.md  # Flash-attention docs
â”‚   â”œâ”€â”€ install_flash_attention.slurm  # ARM64 flash-attn install
â”‚   â”œâ”€â”€ setup_hunyuan_only.sh         # HunyuanVideo setup
â”‚   â”œâ”€â”€ submit_all_hunyuan.sh         # Launch all 50 jobs
â”‚   â””â”€â”€ hunyuan_jobs/          # Individual SLURM scripts (100 files)
â”‚
â”œâ”€â”€ data/                      # Datasets and outputs
â”‚   â”œâ”€â”€ questions/             # Input tasks (50 G-series)
â”‚   â”œâ”€â”€ outputs/               # Generated videos
â”‚   â””â”€â”€ s3_sync.py            # S3 upload/download utility
â”‚
â”œâ”€â”€ scripts/                   # Inference and evaluation
â”‚   â”œâ”€â”€ run_inference.sh
â”‚   â””â”€â”€ run_evaluation.sh
â”‚
â””â”€â”€ VMEvalKit/                 # Video model evaluation toolkit (submodule)
    â”œâ”€â”€ envs/hunyuan-video-i2v/  # Virtual environment
    â”œâ”€â”€ examples/generate_videos.py  # Main inference script
    â”œâ”€â”€ submodules/HunyuanVideo-I2V/ # Tencent's model
    â””â”€â”€ vmevalkit/              # Model wrappers
```

---

## ğŸ¯ Typical Workflow

### 1. Setup (One-time)
```bash
bash jobs/setup_hunyuan_only.sh           # 10 min
sbatch jobs/install_flash_attention.slurm  # 30 min
```

### 2. Run Inference
```bash
# Single task
./jobs/hunyuan_jobs/submit_hunyuan_G1.sh

# All 50 tasks
./jobs/submit_all_hunyuan.sh
```

### 3. Monitor
```bash
squeue -u $USER                          # Job status
find data/outputs -name "*.mp4" | wc -l  # Videos generated
tail -f logs/hunyuan_G1_<JOBID>.out      # Live progress
```

### 4. Upload Results
```bash
module load aws-cli/2.27.49
set -a && source .env && set +a
python data/s3_sync.py upload data/outputs s3://vm-dataset-yijiangli/hunyuan/
```

---

## ğŸ”§ Key Components

### Flash-Attention (ARM64 Optimized)
- **File**: `jobs/install_flash_attention.slurm`
- **What it does**: Compiles flash-attention for ARM64 with GCC 13.2
- **Result**: 40-50% faster inference, 10% less memory
- **Time**: 30 min (first install), 3 sec (cached wheel)

### HunyuanVideo Setup (ARM64 Compatible)
- **File**: `jobs/setup_hunyuan_only.sh`  
- **What it does**: Creates ARM64-compatible environment
- **Key adaptations**:
  - Uses cluster PyTorch 2.7+cu126 (no reinstall)
  - Upgrades pinned versions to wheel-available ones
  - Makes deepspeed optional (training-only dependency)
- **Downloads**: 3 model checkpoints (~40GB total)

### Job Scripts (50 Tasks)
- **Directory**: `jobs/hunyuan_jobs/`
- **Generated by**: `jobs/generate_all_hunyuan_jobs.py`
- **Each job**: 50 videos, 1 GPU, 80GB VRAM, 48hr limit
- **Total**: 2,500 videos across 50 tasks

---

## ğŸ“ˆ Performance

### Video Generation (720p, 129 frames)
| Configuration | Time/Video | Memory | Speedup |
|--------------|------------|--------|---------|
| Standard PyTorch | 5-7 min | 65-70GB | baseline |
| **+ Flash-Attention** | **3-5 min** | **59-65GB** | **40-50%** |

### Cluster Utilization
- **Parallel jobs**: Up to 50 concurrent
- **GPUs utilized**: Depends on cluster availability
- **Throughput**: ~10-15 videos/hour per GPU
- **Total capacity**: ~500-750 videos/hour (50 GPUs)

---

## ğŸ“š Documentation

- **[QUICKSTART.md](QUICKSTART.md)** - Get started in 3 steps
- **[AGENT_HISTORY.md](AGENT_HISTORY.md)** - Complete deployment history and troubleshooting
- **[jobs/FLASH_ATTENTION_SUCCESS.md](jobs/FLASH_ATTENTION_SUCCESS.md)** - Flash-attention installation details
- **[jobs/README.md](jobs/README.md)** - Jobs directory organization

---

## ğŸ› ï¸ Troubleshooting

### Flash-Attention Won't Compile
**Symptoms**: GCC errors, OOM kills  
**Solution**: See [jobs/FLASH_ATTENTION_SUCCESS.md](jobs/FLASH_ATTENTION_SUCCESS.md)

### Jobs Fail with Import Errors
**Symptoms**: `ModuleNotFoundError: No module named 'flash_attn'` or `'deepspeed'`  
**Solution**: 
1. Verify flash-attention: `source VMEvalKit/envs/hunyuan-video-i2v/bin/activate && python -c "import flash_attn"`
2. Check deepspeed patch: `VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/utils/helpers.py` should have try-except

### AWS Credentials Invalid
**Symptoms**: `InvalidAccessKeyId` or `AccessDenied`  
**Solution**: Update `.env` with valid AWS credentials

**For detailed troubleshooting**: See [AGENT_HISTORY.md](AGENT_HISTORY.md#troubleshooting)

---

## ğŸ—ï¸ Architecture

### System Stack
```
Grace-Hopper GH200 (ARM64 + Hopper GPU)
â”œâ”€â”€ CUDA 12.6.1
â”œâ”€â”€ Python 3.12.8 (miniforge3_pytorch/2.7.0)
â”œâ”€â”€ PyTorch 2.7.0.dev20250224+cu126
â””â”€â”€ Flash-Attention 2.7.4.post1 (compiled for sm_90)
```

### Virtual Environment
```
VMEvalKit/envs/hunyuan-video-i2v/
â”œâ”€â”€ --system-site-packages (inherits cluster torch)
â”œâ”€â”€ HunyuanVideo dependencies (ARM64 wheels)
â””â”€â”€ Flash-attention (compiled)
```

### Model Checkpoints
```
VMEvalKit/submodules/HunyuanVideo-I2V/ckpts/
â”œâ”€â”€ hunyuan-video-i2v-720p/      # Main model (~29GB)
â”œâ”€â”€ text_encoder_i2v/             # LLaVA-Llama-3-8B (~16GB)
â””â”€â”€ text_encoder_2/               # CLIP-L (~1.7GB)
```

---

## ğŸ“ Key Learnings

1. **ARM64 requires careful dependency management** - Not all PyPI packages have ARM64 wheels
2. **HPC module systems need explicit handling** - Module loading doesn't always update PATH
3. **Compiler version matrices matter** - GCC 13.x is the sweet spot for CUDA 12.6 + PyTorch 2.7
4. **Flash-attention is worth the effort** - 40-50% speedup justifies compilation complexity

---

## ğŸ“ Citation

If you use this work, please cite:

```bibtex
@software{video_reason_experiments_2026,
  title={Video Reasoning Experiments on Grace-Hopper HPC},
  author={Your Name},
  year={2026},
  note={HunyuanVideo-I2V deployment with flash-attention on ARM64 HPC}
}
```

---

## ğŸ“„ License

Apache License 2.0 - See [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **HunyuanVideo-I2V**: Tencent Hunyuan Team
- **Flash-Attention**: Tri Dao (Dao-AILab)
- **VMEvalKit**: Video Reasoning framework
- **NCSA Delta**: Grace-Hopper HPC resources

---

**Status**: âœ… **Production Ready** - 50 jobs running with flash-attention on ARM64 GH200
